{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "0RU-WO7kdmMM",
    "ExecuteTime": {
     "end_time": "2025-07-17T12:31:08.681349Z",
     "start_time": "2025-07-17T12:31:02.002492Z"
    }
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import AutoTokenizer, AutoModel, pipeline\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import classification_report\n",
    "import copy"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-17 14:31:06.750698: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-17 14:31:06.765436: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1752755466.787205   31618 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1752755466.792555   31618 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1752755466.808183   31618 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752755466.808207   31618 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752755466.808208   31618 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752755466.808209   31618 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-17 14:31:06.813744: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gbapuQUeyE0S",
    "outputId": "7a20781d-293d-4fd2-d7f6-87ff7a0c0102",
    "ExecuteTime": {
     "end_time": "2025-07-17T12:31:08.798774Z",
     "start_time": "2025-07-17T12:31:08.699888Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load dataset"
   ],
   "metadata": {
    "id": "RNUlQOuLfPqH"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "train_df = pd.read_csv('../data/train.txt', header=None, sep=\";\")\n",
    "test_df = pd.read_csv('../data/test.txt', header=None, sep=\";\")\n",
    "val_df = pd.read_csv('../data/validation.txt', header=None, sep=\";\")"
   ],
   "metadata": {
    "id": "Uxc1ekBKd_gW",
    "ExecuteTime": {
     "end_time": "2025-07-17T12:31:08.986661Z",
     "start_time": "2025-07-17T12:31:08.945328Z"
    }
   },
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": [
    "train_df.head()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "SF43L1KUejY5",
    "outputId": "f6aecec8-666a-41fe-c71d-43f4eaa94329",
    "ExecuteTime": {
     "end_time": "2025-07-17T12:31:09.030406Z",
     "start_time": "2025-07-17T12:31:09.015082Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                   0        1\n",
       "0                            i didnt feel humiliated  sadness\n",
       "1  i can go from feeling so hopeless to so damned...  sadness\n",
       "2   im grabbing a minute to post i feel greedy wrong    anger\n",
       "3  i am ever feeling nostalgic about the fireplac...     love\n",
       "4                               i am feeling grouchy    anger"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i didnt feel humiliated</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i can go from feeling so hopeless to so damned...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am feeling grouchy</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": [
    "train_df[1].value_counts()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 303
    },
    "id": "_zxtKO9dxN4C",
    "outputId": "681d77f9-c01f-4c07-dc43-714eb8cfbcc0",
    "ExecuteTime": {
     "end_time": "2025-07-17T12:31:09.100865Z",
     "start_time": "2025-07-17T12:31:09.093343Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1\n",
       "joy         5362\n",
       "sadness     4666\n",
       "anger       2159\n",
       "fear        1937\n",
       "love        1304\n",
       "surprise     572\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "We need to make a encoder for emotions"
   ],
   "metadata": {
    "id": "f3YB8n2KxSw-"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "label_encoder = LabelEncoder()\n",
    "train_labels = label_encoder.fit_transform(train_df[1])\n",
    "test_labels = label_encoder.transform(test_df[1])\n",
    "val_labels = label_encoder.transform(val_df[1])"
   ],
   "metadata": {
    "id": "4gEGhAejwi_f",
    "ExecuteTime": {
     "end_time": "2025-07-17T12:31:09.162774Z",
     "start_time": "2025-07-17T12:31:09.152989Z"
    }
   },
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "source": [
    "print(label_encoder.classes_)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "itwoYT4vx6Yj",
    "outputId": "d4e17e39-752b-49ed-be1c-66e301783077",
    "ExecuteTime": {
     "end_time": "2025-07-17T12:31:09.282785Z",
     "start_time": "2025-07-17T12:31:09.276971Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anger' 'fear' 'joy' 'love' 'sadness' 'surprise']\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "model.eval()\n",
    "\n",
    "# Move to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "train_texts = train_df[0].tolist()\n",
    "test_texts = test_df[0].tolist()\n",
    "val_texts = val_df[0].tolist()\n",
    "\n",
    "def get_embeddings(texts):\n",
    "    batch_size = 32\n",
    "    all_embeddings = []\n",
    "\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        encodings = tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        encodings = {k: v.to(device) for k, v in encodings.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**encodings)\n",
    "            cls_embeddings = outputs.last_hidden_state[:, 0]  # CLS token\n",
    "\n",
    "        all_embeddings.append(cls_embeddings.cpu())\n",
    "\n",
    "    embeds = torch.cat(all_embeddings)\n",
    "    return embeds\n",
    "\n",
    "X_train_tensor = get_embeddings(train_texts)\n",
    "X_test_tensor = get_embeddings(test_texts)\n",
    "X_val_tensor = get_embeddings(val_texts)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MhRiI4OQmroL",
    "outputId": "793cfc5b-15d3-4e21-a1ba-161a6c3d88e4",
    "ExecuteTime": {
     "end_time": "2025-07-17T12:31:39.232103Z",
     "start_time": "2025-07-17T12:31:09.384656Z"
    }
   },
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Dataset & DataLoader"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T12:31:39.262576Z",
     "start_time": "2025-07-17T12:31:39.256612Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y_train_tensor = torch.tensor(train_labels, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(test_labels, dtype=torch.long)\n",
    "y_val_tensor = torch.tensor(val_labels, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model"
   ],
   "metadata": {
    "id": "6asL8Oorx9qL"
   }
  },
  {
   "metadata": {
    "id": "JCiBmf4Jx_hr",
    "ExecuteTime": {
     "end_time": "2025-07-17T12:31:39.317677Z",
     "start_time": "2025-07-17T12:31:39.297664Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class FCNNModel(nn.Module):\n",
    "    def __init__(self, input_size=768, num_classes=6):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 256)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(128, num_classes)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        out = self.fc3(x)\n",
    "        return out"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T12:31:39.408345Z",
     "start_time": "2025-07-17T12:31:39.402917Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train(model, train_loader, optimizer, criterion, device, epochs=10, val_loader=None, patience=5):\n",
    "    model.to(device)\n",
    "\n",
    "    best_val_acc = 0\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            correct += (preds == y_batch).sum().item()\n",
    "            total += y_batch.size(0)\n",
    "\n",
    "        train_acc = correct / total\n",
    "        print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {total_loss:.4f} | Train Acc: {train_acc:.4f}\", end=\"\")\n",
    "\n",
    "        if val_loader:\n",
    "            val_loss, val_acc = evaluate(model, val_loader, criterion, device, return_metrics=True)\n",
    "            print(f\" | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "            # Early stopping logic\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                epochs_no_improve = 0\n",
    "                best_model_wts = model.state_dict()\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "\n",
    "            if epochs_no_improve >= patience:\n",
    "                model.load_state_dict(best_model_wts)\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                torch.save(best_model_wts, \"../models/best_fcnn_model.pth\")\n",
    "                break\n",
    "        else:\n",
    "            print()"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T12:31:39.468864Z",
     "start_time": "2025-07-17T12:31:39.462788Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate(model, data_loader, criterion, device, label_encoder=None, return_metrics=False):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in data_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(y_batch.cpu().numpy())\n",
    "\n",
    "            correct += (preds == y_batch).sum().item()\n",
    "            total += y_batch.size(0)\n",
    "\n",
    "    accuracy = correct / total\n",
    "\n",
    "    if return_metrics:\n",
    "        avg_loss = total_loss / len(data_loader)\n",
    "        return avg_loss, accuracy\n",
    "    else:\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(all_labels, all_preds, target_names=label_encoder.classes_ if label_encoder else None))"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "id": "9xC9zAMz1Ru5",
    "ExecuteTime": {
     "end_time": "2025-07-17T12:31:39.525215Z",
     "start_time": "2025-07-17T12:31:39.516112Z"
    }
   },
   "cell_type": "code",
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "model = FCNNModel()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Train"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T12:32:15.569958Z",
     "start_time": "2025-07-17T12:31:39.585779Z"
    }
   },
   "cell_type": "code",
   "source": "train(model, train_loader, optimizer, criterion, device, epochs=50, val_loader=test_loader, patience=5)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 | Train Loss: 685.7206 | Train Acc: 0.4763 | Val Loss: 1.2225 | Val Acc: 0.5445\n",
      "Epoch 2/50 | Train Loss: 608.1442 | Train Acc: 0.5453 | Val Loss: 1.1279 | Val Acc: 0.5745\n",
      "Epoch 3/50 | Train Loss: 582.1056 | Train Acc: 0.5656 | Val Loss: 1.1026 | Val Acc: 0.5815\n",
      "Epoch 4/50 | Train Loss: 563.9680 | Train Acc: 0.5756 | Val Loss: 1.0664 | Val Acc: 0.5875\n",
      "Epoch 5/50 | Train Loss: 550.1045 | Train Acc: 0.5854 | Val Loss: 1.0673 | Val Acc: 0.5985\n",
      "Epoch 6/50 | Train Loss: 539.7444 | Train Acc: 0.5918 | Val Loss: 1.0519 | Val Acc: 0.6035\n",
      "Epoch 7/50 | Train Loss: 531.3239 | Train Acc: 0.5984 | Val Loss: 1.0433 | Val Acc: 0.6000\n",
      "Epoch 8/50 | Train Loss: 523.1310 | Train Acc: 0.6023 | Val Loss: 1.0242 | Val Acc: 0.6045\n",
      "Epoch 9/50 | Train Loss: 516.2972 | Train Acc: 0.6115 | Val Loss: 1.0323 | Val Acc: 0.6030\n",
      "Epoch 10/50 | Train Loss: 513.7974 | Train Acc: 0.6107 | Val Loss: 1.0425 | Val Acc: 0.6120\n",
      "Epoch 11/50 | Train Loss: 506.5310 | Train Acc: 0.6176 | Val Loss: 1.0102 | Val Acc: 0.6085\n",
      "Epoch 12/50 | Train Loss: 501.1356 | Train Acc: 0.6186 | Val Loss: 1.0172 | Val Acc: 0.6130\n",
      "Epoch 13/50 | Train Loss: 495.7182 | Train Acc: 0.6238 | Val Loss: 1.0264 | Val Acc: 0.6110\n",
      "Epoch 14/50 | Train Loss: 490.9721 | Train Acc: 0.6277 | Val Loss: 0.9991 | Val Acc: 0.6180\n",
      "Epoch 15/50 | Train Loss: 487.8447 | Train Acc: 0.6293 | Val Loss: 1.0161 | Val Acc: 0.6155\n",
      "Epoch 16/50 | Train Loss: 482.9807 | Train Acc: 0.6336 | Val Loss: 1.0456 | Val Acc: 0.6000\n",
      "Epoch 17/50 | Train Loss: 481.1016 | Train Acc: 0.6323 | Val Loss: 1.0242 | Val Acc: 0.6160\n",
      "Epoch 18/50 | Train Loss: 474.8751 | Train Acc: 0.6390 | Val Loss: 1.0189 | Val Acc: 0.6190\n",
      "Epoch 19/50 | Train Loss: 470.6308 | Train Acc: 0.6402 | Val Loss: 1.0001 | Val Acc: 0.6270\n",
      "Epoch 20/50 | Train Loss: 465.4643 | Train Acc: 0.6441 | Val Loss: 1.0049 | Val Acc: 0.6135\n",
      "Epoch 21/50 | Train Loss: 466.0827 | Train Acc: 0.6451 | Val Loss: 0.9884 | Val Acc: 0.6240\n",
      "Epoch 22/50 | Train Loss: 462.0042 | Train Acc: 0.6467 | Val Loss: 0.9900 | Val Acc: 0.6130\n",
      "Epoch 23/50 | Train Loss: 456.5595 | Train Acc: 0.6519 | Val Loss: 1.0057 | Val Acc: 0.6155\n",
      "Epoch 24/50 | Train Loss: 456.8840 | Train Acc: 0.6501 | Val Loss: 1.0522 | Val Acc: 0.5965\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T12:32:15.717487Z",
     "start_time": "2025-07-17T12:32:15.604654Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = FCNNModel()\n",
    "model.load_state_dict(torch.load(\"../models/best_fcnn_model.pth\"))\n",
    "model.to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "evaluate(model, val_loader, criterion, label_encoder=label_encoder, device=device, return_metrics=False)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.54      0.32      0.40       275\n",
      "        fear       0.57      0.48      0.52       212\n",
      "         joy       0.79      0.69      0.73       704\n",
      "        love       0.46      0.32      0.38       178\n",
      "     sadness       0.52      0.84      0.64       550\n",
      "    surprise       0.65      0.16      0.26        81\n",
      "\n",
      "    accuracy                           0.60      2000\n",
      "   macro avg       0.59      0.47      0.49      2000\n",
      "weighted avg       0.62      0.60      0.59      2000\n",
      "\n"
     ]
    }
   ],
   "execution_count": 15
  }
 ]
}
